from difflib import SequenceMatcher

from transformers import AutoModelForCausalLM, AutoTokenizer
import os
import csv 
import time
import json
import argparse

def parse_args():
    parser = argparse.ArgumentParser(description="Evaluate human benchmarks.")
    parser.add_argument("--pred_path", default=r'./eval_human/qwen25-cot-32b.json', help="The path to file containing prediction.")
    parser.add_argument("--fixed_path", default=r'./eval_human/qwen25-cot-32b_fixed.json', help="The path to file containing prediction.")
    
    args = parser.parse_args()
    return args


def normalize_step(step):
    step = step.strip(" \n。，,.!?")
    step = step.lower()
    return step


def split_causal_chain(text):
    if not isinstance(text, list):
        return [normalize_step(x) for x in text.split("→") if x.strip()]
    else:
        return [normalize_step(x) for x in text[0].split("→") if x.strip()]


def is_semantic_match(pred_step, gt_step, threshold=0.75):
    return SequenceMatcher(None, pred_step, gt_step).ratio() >= threshold

def fuzzy_precision_recall_f1(pred_steps, gt_steps):
    matched_pred = set()
    matched_gt = set()

    for i, p in enumerate(pred_steps):
        for j, g in enumerate(gt_steps):
            if is_semantic_match(p, g):
                matched_pred.add(i)
                matched_gt.add(j)
                break

    precision = len(matched_pred) / len(pred_steps) if pred_steps else 0
    recall = len(matched_gt) / len(gt_steps) if gt_steps else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0
    return precision, recall, f1

def lcs_length(a, b):
    n, m = len(a), len(b)
    dp = [[0]*(m+1) for _ in range(n+1)]
    for i in range(n):
        for j in range(m):
            if is_semantic_match(a[i], b[j]):
                dp[i+1][j+1] = dp[i][j]+1
            else:
                dp[i+1][j+1] = max(dp[i+1][j], dp[i][j+1])
    return dp[-1][-1]

def lcs_order_score(pred_steps, gt_steps):
    return lcs_length(pred_steps, gt_steps) / len(gt_steps) if gt_steps else 0


#############################MLLMs#############################
model_name = "/pre_hfs/Qwen2.5-VL-72B-Instruct"

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto",
    attn_implementation="flash_attention_2",
)
tokenizer = AutoTokenizer.from_pretrained(model_name)
#############################MLLMs#############################


def fix_json_lines(input_path: str, output_path: str):
    fixed_list = []

    with open(input_path, 'r', encoding='utf-8') as f:
        for idx, line in enumerate(f, 1):
            line = line.strip()
            if not line:
                continue
            try:
                fixed_list.append(json.loads(line))
            except json.JSONDecodeError as e:
                print('error')
                
    with open(output_path, 'w', encoding='utf-8') as fout:
        json.dump(fixed_list, fout, ensure_ascii=False, indent=2)
        


def main():
    args = parse_args()
    
    fix_json_lines(args.pred_path, args.fixed_path)

    # === Load result file ===
    with open(args.fixed_path, "r", encoding="utf-8") as f:
        res = json.load(f)

        # === Evaluate each sample ===
        for item in res:
            pred = split_causal_chain(item["pred"])
            gt_answers = list({normalize_step(ans) for ans in item["answer"]})
            
            prompt = (
                "Please determine whether the causal explanation generated by the model is semantically consistent with the reference causal chain. "
                "Model's causal explanation: {} "
                "Reference causal chain: {} "
                "Please provide a consistency score (0–5) directly, and do not include any explanation."
            ).format(pred, gt_answers)
            

            messages = [
                {"role": "system", "content": "You are Qwen, created by Alibaba Cloud. You are a helpful assistant."},
                {"role": "user", "content": prompt}
            ]
            text = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

            generated_ids = model.generate(
                **model_inputs,
                max_new_tokens=512
            )
            generated_ids = [
                output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
            ] 

            gpt_score = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0] 
            
            precision, recall, f1 = fuzzy_precision_recall_f1(pred, gt_answers)
            
            
            order_score = lcs_order_score(pred, gt_answers)
            
            final_score = 0.5 * f1 + 0.3 * order_score + 0.5 * float(gpt_score) / 5
            print(final_score)
            
if __name__ == '__main__':
    main()